{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "295276a2-4d90-466c-a63f-625da19f1e6c",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border:none; font-family: Arial, sans-serif;\">\n",
    "  <tr>\n",
    "    <td style=\"width:120px;\">\n",
    "      <img src=\"./images/logo.png\" alt=\"Logo ENSAJ\" width=\"100\" style=\"display:block; margin:auto;\" />\n",
    "    </td>\n",
    "    <td style=\"vertical-align: middle; text-align: left; padding-left: 15px;\">\n",
    "      <h1 style=\"margin-bottom:4px; font-weight: bold; color: #2E4053;\">Intention d’Achat des Acheteurs en Ligne</h1>\n",
    "      <h2 style=\"margin-top:0; margin-bottom:6px; font-weight: normal; color: #34495E;\">Entraînement des Modèles SVM et Arbre de Décision</h2>\n",
    "      <p style=\"margin:0; font-weight: 600; color: #5D6D7E;\">Ossama ETTAQAFI</p>\n",
    "      <p style=\"margin:0; font-style: italic; color: #7F8C8D;\">ENSAJ, Master SDIA - Année universitaire 2024-2025</p>\n",
    "      <p style=\"margin-top:10px; font-size: 0.9em; color: #7F8C8D;\">\n",
    "        Source des données : <a href=\"https://www.kaggle.com/datasets/imakash3011/online-shoppers-purchasing-intention-dataset/data\" target=\"_blank\">Kaggle - Online Shoppers Purchasing Intention</a>\n",
    "      </p>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "## Objectif du Notebook\n",
    "\n",
    "Ce notebook présente la construction, l’entraînement et l’évaluation de plusieurs modèles de machine learning, notamment :  \n",
    "- **Support Vector Machine (SVM)**  \n",
    "- **Arbre de Décision**  \n",
    "- **Random Forest**  \n",
    "- **Régression Logistique**\n",
    "\n",
    "L’objectif est de prédire l’intention d’achat des visiteurs en ligne à partir de leurs comportements de navigation.\n",
    "\n",
    "---\n",
    "\n",
    "## Contexte du Projet\n",
    "\n",
    "Les données ont été prétraitées et nettoyées dans les étapes précédentes. Nous utilisons ici deux algorithmes supervisés simples mais efficaces pour comprendre la performance prédictive sur ce dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs du Notebook\n",
    "\n",
    "- Charger les données réduites (`reduced_data.csv`)  \n",
    "- Séparer les données en ensembles d’entraînement (`train_set.csv`) et de test (`test_set.csv`)  \n",
    "- Entraîner les modèles : SVM, Arbre de Décision, Random Forest, Régression Logistique  \n",
    "- Évaluer les modèles avec : \n",
    "  - Accuracy\n",
    "  - F1-score\n",
    "  - Matrice de confusion\n",
    "  - Courbe ROC + AUC  \n",
    "- Comparer les performances et exporter les prédictions pour analyse et visualisation\n",
    "\n",
    "## Environnement\n",
    "\n",
    "- Python 3.x  \n",
    "- Bibliothèques : pandas, numpy, scikit-learn, matplotlib, seaborn, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3a986f-329d-40f8-a8c0-ceba4573c675",
   "metadata": {},
   "source": [
    "**Alors, on commence notre :**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad3d52f-8cf2-4fc6-a3eb-08d91b0d6acd",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">L'entraînement et l’évaluation des models</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb09b5-1820-47aa-89e1-8129571423ec",
   "metadata": {},
   "source": [
    "## 1. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3859e5e8-1cac-466b-8e17-b9cd12476eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# 1. Charger les datasets\n",
    "train_set = pd.read_csv(\"train_set.csv\")\n",
    "test_set = pd.read_csv(\"test_set.csv\")\n",
    "\n",
    "# 2. Séparer les features (X) et la cible (y)\n",
    "X_train = train_set.drop(\"Revenue\", axis=1)\n",
    "y_train = train_set[\"Revenue\"]\n",
    "\n",
    "X_test = test_set.drop(\"Revenue\", axis=1)\n",
    "y_test = test_set[\"Revenue\"]\n",
    "\n",
    "# 3. Initialiser et entraîner le modèle SVM avec gestion du déséquilibre\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42, class_weight='balanced', probability=True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Prédictions\n",
    "y_pred = svm_model.predict(X_test)\n",
    "y_pred_proba = svm_model.predict_proba(X_test)[:, 1]  # Probabilité pour la classe positive\n",
    "\n",
    "# 5. Évaluation\n",
    "print(\"Évaluation du modèle SVM\")\n",
    "print(f\"Accuracy  : {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-score  : {f1_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "print(\"\\nClassification Report :\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# 6. Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "plt.title(\"Matrice de confusion - SVM\")\n",
    "plt.xlabel(\"Prédit\")\n",
    "plt.ylabel(\"Réel\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Courbe ROC et AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"Taux de Faux Positifs (False Positive Rate)\")\n",
    "plt.ylabel(\"Taux de Vrais Positifs (True Positive Rate)\")\n",
    "plt.title(\"Courbe ROC - SVM\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. Exporter les prédictions dans un CSV\n",
    "df_pred = pd.DataFrame({\n",
    "    \"Actual\": y_test.reset_index(drop=True),\n",
    "    \"Predicted\": y_pred,\n",
    "    \"Predicted_Probability\": y_pred_proba\n",
    "})\n",
    "\n",
    "df_pred.to_csv(\"predictions_svm.csv\", index=False)\n",
    "print(\"Prédictions SVM exportées dans 'predictions_svm.csv'.\")\n",
    "\n",
    "# 9. Sauvegarder le modèle SVM dans le dossier 'models'\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model_path = \"./models/svm_model.joblib\"\n",
    "joblib.dump(svm_model, model_path)\n",
    "print(f\"Modèle SVM sauvegardé dans '{model_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f0680-3e5b-4c8f-b863-dc90c20dedf5",
   "metadata": {},
   "source": [
    "#### Interprétation\n",
    "Le SVM n’est pas super précis dans l’ensemble, mais il a très bien détecté les acheteurs.\n",
    "Même si le modèle se trompe souvent en pensant qu’un client va acheter alors que ce n’est pas vrai, il rate très peu de vrais acheteurs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f385bd6b-ab1b-4b23-b88d-47314ca7b80f",
   "metadata": {},
   "source": [
    "## 2. Arbre de Décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ce1e0-ab79-4987-a3a2-a9bfe672f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# 1. Charger les datasets\n",
    "train_set = pd.read_csv(\"train_set.csv\")\n",
    "test_set = pd.read_csv(\"test_set.csv\")\n",
    "\n",
    "# 2. Séparer les features (X) et la cible (y)\n",
    "X_train = train_set.drop(\"Revenue\", axis=1)\n",
    "y_train = train_set[\"Revenue\"]\n",
    "X_test = test_set.drop(\"Revenue\", axis=1)\n",
    "y_test = test_set[\"Revenue\"]\n",
    "\n",
    "# 3. Initialiser et entraîner le modèle Arbre de décision avec gestion du déséquilibre\n",
    "dt_model = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Prédictions (classes et probabilités)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "y_prob_dt = dt_model.predict_proba(X_test)[:, 1]  # Probabilité pour la classe positive\n",
    "\n",
    "# 5. Évaluation\n",
    "print(\"Évaluation du modèle : Arbre de décision (balanced)\")\n",
    "print(f\"Accuracy  : {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"F1-score  : {f1_score(y_test, y_pred_dt, zero_division=0):.4f}\")\n",
    "print(\"\\nClassification Report :\")\n",
    "print(classification_report(y_test, y_pred_dt, zero_division=0))\n",
    "\n",
    "# 6. Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred_dt)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "plt.title(\"Matrice de confusion - Arbre de décision\")\n",
    "plt.xlabel(\"Prédit\")\n",
    "plt.ylabel(\"Réel\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Courbe ROC et AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob_dt)\n",
    "auc_score = roc_auc_score(y_test, y_prob_dt)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {auc_score:.4f})\", color='darkorange', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel(\"Taux de faux positifs (FPR)\")\n",
    "plt.ylabel(\"Taux de vrais positifs (TPR)\")\n",
    "plt.title(\"Courbe ROC - Arbre de décision\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 8. Exporter les prédictions avec les données réelles et les features\n",
    "df_pred_dt = X_test.reset_index(drop=True).copy()\n",
    "df_pred_dt['Actual'] = y_test.reset_index(drop=True)\n",
    "df_pred_dt['Prediction'] = y_pred_dt\n",
    "df_pred_dt['Probability_Positive'] = y_prob_dt\n",
    "\n",
    "df_pred_dt.to_csv('predictions_decision_tree.csv', index=False)\n",
    "print(\"Prédictions Arbre de décision exportées dans 'predictions_decision_tree.csv'.\")\n",
    "\n",
    "# 9. Sauvegarder le modèle dans 'models/'\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model_path = \"models/decision_tree_model.joblib\"\n",
    "joblib.dump(dt_model, model_path)\n",
    "print(f\"Modèle Arbre de décision sauvegardé dans '{model_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49837465-2535-4aac-a38d-6b389cbec7cd",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "L’arbre de décision est assez bon sur l’ensemble (75% de précision), mais il reconnaît mal les acheteurs (seulement 29% de rappel).\n",
    "Il préfère toujours dire qu’un client n’achètera pas, même quand ce n’est pas vrai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef9733-d4b9-44ae-a068-d67c4542844e",
   "metadata": {},
   "source": [
    "## 3. Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a6dd9-2e46-424f-a9c0-24c6d8b9bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# 1. Charger les datasets\n",
    "train_set = pd.read_csv(\"train_set.csv\")\n",
    "test_set = pd.read_csv(\"test_set.csv\")\n",
    "\n",
    "# 2. Séparer les features (X) et la cible (y)\n",
    "X_train = train_set.drop(\"Revenue\", axis=1)\n",
    "y_train = train_set[\"Revenue\"]\n",
    "\n",
    "X_test = test_set.drop(\"Revenue\", axis=1)\n",
    "y_test = test_set[\"Revenue\"]\n",
    "\n",
    "# 3. Initialiser et entraîner le modèle Régression Logistique avec gestion du déséquilibre\n",
    "log_model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Prédictions (classes et probabilités)\n",
    "y_pred_log = log_model.predict(X_test)\n",
    "y_prob_log = log_model.predict_proba(X_test)[:, 1]  # Probabilité pour la classe positive\n",
    "\n",
    "# 5. Évaluation\n",
    "print(\"Évaluation du modèle : Régression Logistique (balanced)\")\n",
    "print(f\"Accuracy  : {accuracy_score(y_test, y_pred_log):.4f}\")\n",
    "print(f\"F1-score  : {f1_score(y_test, y_pred_log, zero_division=0):.4f}\")\n",
    "print(\"\\nClassification Report :\")\n",
    "print(classification_report(y_test, y_pred_log, zero_division=0))\n",
    "\n",
    "# 6. Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred_log)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "plt.title(\"Matrice de confusion - Régression Logistique\")\n",
    "plt.xlabel(\"Prédit\")\n",
    "plt.ylabel(\"Réel\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Courbe ROC et AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob_log)\n",
    "auc_score = roc_auc_score(y_test, y_prob_log)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {auc_score:.4f})\", color='darkorange', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel(\"Taux de faux positifs (FPR)\")\n",
    "plt.ylabel(\"Taux de vrais positifs (TPR)\")\n",
    "plt.title(\"Courbe ROC - Régression Logistique\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 8. Exporter les prédictions avec les données réelles et les features\n",
    "df_pred_log = X_test.reset_index(drop=True).copy()\n",
    "df_pred_log['Actual'] = y_test.reset_index(drop=True)\n",
    "df_pred_log['Prediction'] = y_pred_log\n",
    "df_pred_log['Probability_Positive'] = y_prob_log\n",
    "\n",
    "df_pred_log.to_csv('predictions_logistic_regression.csv', index=False)\n",
    "print(\"Prédictions Régression Logistique exportées dans 'predictions_logistic_regression.csv'.\")\n",
    "\n",
    "# 9. Sauvegarder le modèle dans 'models/'\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model_path = \"./models/logistic_regression_model.joblib\"\n",
    "joblib.dump(log_model, model_path)\n",
    "print(f\"Modèle de Régression Logistique sauvegardé dans '{model_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e472a5ac-975e-4534-bf59-39bd8ca1a919",
   "metadata": {},
   "source": [
    "#### Interprétation\n",
    "Le modèle est plutôt bon pour identifier les visiteurs qui n’achètent pas (précision élevée sur la classe 0), mais il a plus de difficulté avec ceux qui achètent. Il détecte environ 69% des acheteurs (rappel correct), mais sa précision sur cette classe est faible, ce qui signifie qu’il prédit souvent des achats qui ne se réalisent pas (faux positifs). C’est un comportement fréquent avec des données déséquilibrées, où les acheteurs sont minoritaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77b1a59-951d-4760-885c-71659c399de6",
   "metadata": {},
   "source": [
    "## 4. Forêt Aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a7941-3a00-4deb-8320-3a121aa80706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# 1. Charger les datasets\n",
    "train_set = pd.read_csv(\"train_set.csv\")\n",
    "test_set = pd.read_csv(\"test_set.csv\")\n",
    "\n",
    "# 2. Séparer les features (X) et la cible (y)\n",
    "X_train = train_set.drop(\"Revenue\", axis=1)\n",
    "y_train = train_set[\"Revenue\"]\n",
    "\n",
    "X_test = test_set.drop(\"Revenue\", axis=1)\n",
    "y_test = test_set[\"Revenue\"]\n",
    "\n",
    "# 3. Initialiser et entraîner le modèle Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,        # nombre d'arbres\n",
    "    max_depth=None,          # profondeur maximale (None = pas de limite)\n",
    "    class_weight='balanced', # gestion du déséquilibre\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Prédictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]  # probabilité classe positive\n",
    "\n",
    "# 5. Évaluation\n",
    "print(\"Évaluation du modèle Random Forest\")\n",
    "print(f\"Accuracy  : {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-score  : {f1_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report :\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 6. Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "plt.title(\"Matrice de confusion - Random Forest\")\n",
    "plt.xlabel(\"Prédit\")\n",
    "plt.ylabel(\"Réel\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Courbe ROC et AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"Taux de Faux Positifs (False Positive Rate)\")\n",
    "plt.ylabel(\"Taux de Vrais Positifs (True Positive Rate)\")\n",
    "plt.title(\"Courbe ROC - Random Forest\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. Exporter les prédictions dans un CSV\n",
    "df_pred = pd.DataFrame({\n",
    "    \"Actual\": y_test.reset_index(drop=True),\n",
    "    \"Predicted\": y_pred,\n",
    "    \"Predicted_Probability\": y_pred_proba\n",
    "})\n",
    "\n",
    "df_pred.to_csv(\"predictions_rf.csv\", index=False)\n",
    "print(\"Prédictions Random Forest exportées dans 'predictions_rf.csv'.\")\n",
    "\n",
    "# 9. Sauvegarder le modèle Random Forest dans le dossier 'models'\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model_path = \"./models/random_forest_model.joblib\"\n",
    "joblib.dump(rf_model, model_path)\n",
    "print(f\"Modèle Random Forest sauvegardé dans '{model_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60944e9d-9afc-4e55-8308-5c43bfb1dcd3",
   "metadata": {},
   "source": [
    "#### Interprétation\n",
    "C’est le modèle qui a la meilleure précision globale (80%), mais comme l’arbre de décision, il ne détecte pas bien les acheteurs (rappel faible de 22%).\n",
    "Il est très fort pour dire quand quelqu’un n’achètera pas, mais il rate beaucoup de vrais acheteurs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
